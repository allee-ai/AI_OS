# React Chat App â†’ Nola Stimuli Channel

A local-first AI chat application that serves as a **communication stimuli channel** for [Nola](Nola/README.md) - a personal AI with hierarchical state management. Built with React + TypeScript frontend and FastAPI backend, connected to Nola's Hierarchical Experiential Attention (HEA) system.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![Node](https://img.shields.io/badge/node-18+-green.svg)

## ğŸ§  What is This?

This chat interface is one of several **stimuli channels** that feed into Nola's cognitive system:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STIMULI CHANNELS                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  React Chat  â”‚    Twilio    â”‚    Email     â”‚    CLI     â”‚
â”‚  (this app)  â”‚   (future)   â”‚   (future)   â”‚  (exists)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              NOLA - Hierarchical State                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Context Manager (HEA)                                   â”‚
â”‚  â”œâ”€â”€ L1: Realtime (~10 tokens) - Quick responses        â”‚
â”‚  â”œâ”€â”€ L2: Conversational (~50 tokens) - Default          â”‚
â”‚  â””â”€â”€ L3: Analytical (~200 tokens) - Deep context        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Identity Thread                                         â”‚
â”‚  â”œâ”€â”€ machineID.json â†’ identity.json â†’ Nola.json         â”‚
â”‚  â””â”€â”€ userID.json â”€â”€â”˜                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Ollama (Local LLM)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

- **Hierarchical Context** - Automatic L1/L2/L3 context escalation based on message analysis
- **Real-time Chat** - WebSocket-based streaming responses
- **Local AI** - Powered by Ollama (runs entirely on your machine)
- **Conversation Persistence** - Stored in `Nola/Stimuli/conversations/`
- **Modern Stack** - React 18 + TypeScript + Vite + FastAPI
- **User-Owned Data** - Your conversations, your machine, no cloud

## ğŸš€ Quick Start

### Option 1: One-Command Start (Recommended)

**macOS/Linux:**
```bash
git clone https://github.com/YOUR_USERNAME/react-chat-app.git
cd react-chat-app
chmod +x start.sh
./start.sh
```

**Windows:**
```cmd
git clone https://github.com/YOUR_USERNAME/react-chat-app.git
cd react-chat-app
start.bat
```

The script will:
1. Check/install prerequisites
2. Start Ollama if needed
3. Install dependencies
4. Start backend & frontend
5. Open browser automatically

### Option 2: Docker

```bash
# Ensure Ollama is running on host
ollama serve

# Start with Docker Compose
chmod +x start-docker.sh
./start-docker.sh
```

### Option 3: Manual Setup

#### Prerequisites

1. **Ollama** - https://ollama.ai
   ```bash
   # Install Ollama, then:
   ollama serve
   ollama pull llama3.2:3b  # or your preferred model
   ```

2. **Python 3.11+**
   ```bash
   python3 --version
   ```

3. **Node.js 18+**
   ```bash
   node --version
   ```

#### Setup

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/react-chat-app.git
cd react-chat-app

# Backend setup
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r backend/requirements.txt

# Frontend setup
cd frontend
npm install
cd ..

# Start backend (Terminal 1)
cd backend
uvicorn main:app --reload --port 8000

# Start frontend (Terminal 2)
cd frontend
npm run dev
```

Open http://localhost:5173 in your browser.

## ğŸ“ Project Structure

```
React_Demo/
â”œâ”€â”€ Nola/                    # ğŸ§  The brain - hierarchical state system
â”‚   â”œâ”€â”€ agent.py            # Thread-safe singleton agent
â”‚   â”œâ”€â”€ contract.py         # Metadata protocol
â”‚   â”œâ”€â”€ Nola.json           # Global runtime state
â”‚   â”œâ”€â”€ identity_thread/    # Identity hierarchy
â”‚   â”‚   â”œâ”€â”€ identity.json   # Aggregated identity
â”‚   â”‚   â”œâ”€â”€ machineID/      # Machine context module
â”‚   â”‚   â””â”€â”€ userID/         # User context module
â”‚   â””â”€â”€ Stimuli/
â”‚       â”œâ”€â”€ conversations/  # ğŸ’¬ Chat history stored here
â”‚       â””â”€â”€ comms/          # Future: Twilio, email modules
â”‚
â”œâ”€â”€ react-chat-app/         # ğŸ’» This stimuli channel
â”‚   â”œâ”€â”€ backend/            # FastAPI server
â”‚   â”‚   â”œâ”€â”€ main.py         # App entry point
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ agent_service.py  # â­ Nola integration + HEA
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â”œâ”€â”€ chat.py     # REST endpoints
â”‚   â”‚       â””â”€â”€ websockets.py
â”‚   â””â”€â”€ frontend/           # React + Vite app
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ components/Chat/
â”‚           â”œâ”€â”€ hooks/
â”‚           â””â”€â”€ services/
â”‚
â”œâ”€â”€ docs/                   # ğŸ“š Theory & evaluation
â”‚   â”œâ”€â”€ concept_attention_theory.md
â”‚   â””â”€â”€ tests.md
â”‚
â””â”€â”€ .github/               # ğŸ‘¥ Contributor infrastructure
    â”œâ”€â”€ agents/            # AI agent profiles
    â””â”€â”€ ISSUE_TEMPLATE/
```

## ğŸ”§ Configuration

### Environment Variables

Create `.env` in `react-chat-app/backend/`:
```env
CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173
HOST=0.0.0.0
PORT=8000
DEBUG=true
```

### Ollama Models

Nola uses these models (configure in `Nola/agent.py`):
- `gpt-oss:20b-cloud` (primary)
- `llama3.2:3b` (fallback)
- `mistral:7b` (fallback)

### Context Levels

The chat automatically selects context depth based on your message:

| Message Type | Context Level | Tokens | Example |
|--------------|---------------|--------|---------|
| Casual | L1 (realtime) | ~10 | "Hi!", "Thanks" |
| Substantive | L2 (conversational) | ~50 | "I'm stressed about work" |
| Analytical | L3 (analytical) | ~200 | "Analyze my productivity patterns" |

## ğŸ“¡ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check |
| `/api/chat/message` | POST | Send message |
| `/api/chat/history` | GET | Get chat history |
| `/api/chat/agent/status` | GET | Agent status |
| `/api/chat/clear` | POST | Clear history |
| `/ws` | WebSocket | Real-time chat |

## ğŸ› Troubleshooting

### "Ollama not found"
```bash
# Install Ollama from https://ollama.ai
# Then run:
ollama serve
```

### "Connection refused" / Network error
```bash
# Check if backend is running
curl http://localhost:8000/health

# Check if frontend is running
curl http://localhost:5173
```

### "Model not found"
```bash
# Pull required models
ollama pull llama3.2:3b
ollama pull mistral:7b
```

### Port already in use
```bash
# macOS/Linux
lsof -ti:8000 | xargs kill -9
lsof -ti:5173 | xargs kill -9

# Windows
netstat -ano | findstr :8000
taskkill /PID <PID> /F
```

## ğŸ› ï¸ Development

### Backend

```bash
cd backend
uvicorn main:app --reload --port 8000
# API docs at http://localhost:8000/docs
```

### Frontend

```bash
cd frontend
npm run dev      # Development server
npm run build    # Production build
npm run preview  # Preview production build
```

### Type Checking

```bash
cd frontend
npm run build  # Runs TypeScript compiler
```

## ğŸ“¦ Building for Production

### Docker

```bash
docker-compose up --build
```

### Manual Build

```bash
# Backend - runs as-is with uvicorn
cd backend
uvicorn main:app --host 0.0.0.0 --port 8000

# Frontend - build static files
cd frontend
npm run build
# Serve dist/ with any static server
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing`)
5. Open a Pull Request

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai) - Local LLM runtime
- [FastAPI](https://fastapi.tiangolo.com) - Modern Python web framework
- [Vite](https://vitejs.dev) - Next-gen frontend tooling
- [React](https://react.dev) - UI library
