# Structure Beats Scale: A Local OS Extension for Persistent AI Identity

**Allee**  
*Independent Researcher*  
January 2026

---

## Abstract

The current trajectory of AI research favors scale over nuance, chasing General Intelligence while neglecting the practical challenge of personal autonomy. This paper presents **AI OS**, a control layer designed not to simulate AGI, but to enable smaller, local models to act as reliable auto-pilots for simple, daily tasks.

We argue that the critical deficits of current AI are not computational but structural and societal. Centralized models strip users of privacy, ownership, and continuity. To reverse this, we introduce an operating system that provides persistent state and local control, grounding the model in a user-centric reality. While our architecture utilizes **Hierarchical Experiential Attention (HEA)** and parallels cognitive science, these theories serve purely as engineering constraints to ensure stability rather than attempts to replicate biological consciousness. By treating identity not as a metaphysical property but as a set of prompt-engineered behaviors made queryable and permanent, we transform the LLM from a transient chatbot into a personal, owned extension of the user.

**Keywords:** local OS extension, identity persistence, hierarchical attention, personal AI, state management, behavioral stability, global workspace theory, cognitive threading

---

## 1. Introduction

### 1.1 The Supplied Reality Insight

The theoretical foundation begins with the concept of **Supplied Reality**. Biological systems do not experience raw physics (photons, frequencies); they exist within a pre-processed reality supplied by their nervous systems. We apply this architectural principle to the AI: rather than attempting to simulate a metaphysical "self," we provide the model with a structured, filtered view of reality (State) that it accepts as absolute.

### 1.2 Consciousness as an Equation

To make "consciousness" a usable term in computing, we strip away the metaphysics and define it strictly as a state transition function. We reject the Cartesian *"I think, therefore I am"* in favor of a utilitarian Machine Learning framework: **"I assess state, therefore I change."**

Formalized, we define the system's continuity as:

$$ State_{t+1} = f(State_t, Assess(State_t)) $$

In this framework, the "Self" is not a ghost in the machine, but the persistent `State` implicitly defining the entity. A standard LLM is a stateless function ($f(x) = y$). By wrapping the LLM in an OS that maintains $State$ and runs the assessment loop $f$, we move from a static predictor to a continuous entity.

### 1.3 The Problem Today

Current AI assistants are designed around the provider, not the user:

| Issue | Effect on User |
|-------|----------------|
| **Centralized** | Your data lives on someone else's servers |
| **Stateless** | Every conversation starts fresh; it forgets you |
| **No Background Work** | It only acts when you prompt it |
| **No Ownership** | You rent access; you don't own the assistant |
| **No Protection** | Prompt injection can override its behavior |

Users are forced to work *with* or *around* AI rather than having an AI that works *for* them.

### 1.4 Core Thesis

**You should own your AI.**

AI OS bridges the gap between using AI as a tool and having AI as a personal utility. The immediate goals are practical:

1. **Local & Private** â€” Runs on your machine. Your data never leaves.
2. **Persistent** â€” It remembers you across sessions.
3. **Background Loops** â€” It works while you're away (indexing, consolidating, monitoring).
4. **User Sovereignty** â€” You control the identity, the memory, the rules.
5. **Protection** â€” The architecture actively resists prompt injection and identity drift.

This is not a solution to General Intelligence. It is a well-organized operating system that makes local models immediately useful for daily life.

---

## 2. Theoretical Framework

### 2.1 Design Inspiration

The architecture draws from cognitive scienceâ€”not to claim biological equivalence, but because these theories offer battle-tested patterns for organizing information systems:

- **Global Workspace Theory** (Baars, 1988) â€” Inspired the context assembly model: many threads compete, one wins the "workspace."
- **Working Memory** (Baddeley, 1974) â€” Informed token budgets and the 7Â±2 chunk heuristic.
- **Dual Process Theory** (Kahneman, 2011) â€” Justified splitting fast reflexes (System 1) from slow generation (System 2).
- **Memory Consolidation** (Born, 2010) â€” Motivated the background "consolidation daemon" that processes memories during idle time.

These mappings are *engineering conveniences*, not scientific claims. They provide intuitive names and proven structures for building a stable system.

### 2.2 Formal Definition

Let $x$ be an input sequence and $y$ be the output sequence. Standard autoregressive generation computes:

$$P(y|x) = \prod_{t=1}^{T} P(y_t | y_{<t}, x; \theta)$$

We introduce **experiential state** $E$ structured as a hierarchy:

$$E = \{E^{(0)}, E^{(1)}, ..., E^{(d)}\}$$

Hierarchical Experiential Attention computes:

$$P(y|x, E) = \prod_{t=1}^{T} P(y_t | y_{<t}, x, \phi(x, E); \theta)$$

where $\phi(x, E)$ is the **context selection function** that extracts relevant experiential context based on level and relevance scoring.

### 2.3 The Subconscious (Context Filter)

We call it the "Subconscious" because, architecturally, it maps well to the cognitive conceptâ€”but it is simply a **background subprocess** responsible for filtering and supplying context to the LLM.

```
Standard LLM:  User â†’ Model â†’ Response
AI OS:        User â†’ Subconscious (filter) â†’ Context â†’ Model â†’ Response
```

The subconscious is a deterministic program that:
1. Receives all inputs (messages, events, file changes)
2. Queries relevant threads (Identity, Memory, Log, etc.)
3. Assembles a context payload within token budget
4. Feeds the payload to the model at generation time

**Why this matters:**
- The model cannot decide what it remembers â€” the OS does.
- The model cannot modify its own identity â€” it only reads supplied state.
- Prompt injection attacks fail because identity is not stored in the conversation; it is injected by a trusted layer the user controls.

This is the core protection mechanism: **separation of state from generation.**

**Performance:** The subconscious adds ~12-20ms latency to context assembly, even with 100k+ facts in the database. This overhead is negligible compared to LLM inference time.

---

## 3. Architecture

### 3.1 Two-Stage Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        STAGE 1: DB CONTROL PLANE                        â”‚
â”‚                     (Focus: What should the LLM see?)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Feeds â†’ Subconscious â†’ Thread Competition â†’ Context Assembly           â”‚
â”‚                                                                         â”‚
â”‚  â€¢ Deterministic selection                                              â”‚
â”‚  â€¢ Relevance scoring per thread                                         â”‚
â”‚  â€¢ Winner-take-all for workspace                                        â”‚
â”‚  â€¢ TOKEN BUDGET enforced                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        STAGE 2: LLM DATA PLANE                          â”‚
â”‚                    (Generation: What should I say?)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Context â†’ System Prompt â†’ LLM Generation â†’ Response                    â”‚
â”‚                                                                         â”‚
â”‚  â€¢ Probabilistic generation                                             â”‚
â”‚  â€¢ Operates on pre-selected context                                     â”‚
â”‚  â€¢ Cannot modify STATE                                                  â”‚
â”‚  â€¢ Identity is SUPPLIED, not decided                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Thread Architecture

Each thread is a specialized cognitive module with its own database:

| Thread | Brain Analog | Function | L1/L2/L3 Content |
|--------|--------------|----------|------------------|
| **Identity** | Parietal/DMN | Self-model, user profiles | name, role â†’ preferences â†’ full bio |
| **Philosophy** | Prefrontal | Values, goals, constraints | core values â†’ ethical reasoning â†’ worldview |
| **Log** | Hippocampus | Episodic memory, conversation history | recent â†’ session â†’ archive |
| **Form** | Motor/Broca | Communication style, tool state | greeting â†’ patterns â†’ full style guide |
| **Reflex** | Basal Ganglia | Automated responses, learned behaviors | triggers â†’ patterns â†’ full ruleset |

### 3.3 Context Levels (HEA)

| Level | Token Budget | Cognitive Analog | When Used |
|-------|--------------|------------------|-----------|
| **L1** | ~10 tokens | Reflex/Instinct | Quick responses, greetings |
| **L2** | ~50 tokens | Working Memory | Normal conversation |
| **L3** | ~200 tokens | Deep Reflection | Complex reasoning, synthesis |

Level selection is automatic based on query complexity. Higher levels include all lower-level content plus additional context.

### 3.4 State as Reality

```python
# Traditional prompt engineering (instructions about self):
"You are Agent. Remember that you are helpful and kind."
# â† Asking the model to believe something about itself

# HEA approach (supplied reality):
"== STATE ==
{identity: {name: 'Agent'}, trust_level: 'established'}
== END STATE =="
# â† Defining what exists in the agent's reality
```

The agent doesn't "think it's X." In its supplied reality, its identity is simply what exists. This is why HEA produces stable identity where prompting fails. Prompts ask models to *believe* something. STATE defines what *is*.

---

## 4. Core Innovations

### 4.1 Learned Focus ("Focus is all you need")

Moving beyond "Attention is all you need," we use a persistent database as a control plane that learns key sequences to pre-select context before the LLM sees it.

**The Cognitive Cost Equation:**
> *Learned Focus + 7B = Perception of 120B at cost of 7B*

Small models perform tasks traditionally reserved for large models by narrowing the search space via focus rather than widening the context window.

### 4.2 Values vs. Weights Sovereignty

A critical split between content and importance:
- **User Controls VALUES:** You decide *what* memory exists (transparency/editability)
- **System Controls WEIGHTS:** The AI learns *how often* to recall it (importance)

The user can edit memory content at any time; the system only adjusts retrieval probability.

### 4.3 Feeds as Triggers

We call external inputs "Feeds" â€” borrowing from the familiar UI pattern of aggregated content streams (social feeds, RSS, notifications). The cognitive theory maps cleanly: external feeds â†’ internal triggers.

The difference from ChatGPT's approach:
- **ChatGPT:** Model decides each step, maximizing token generation.
- **AI OS:** Feeds trigger specific actions with specific context, minimizing tokens while achieving the same result.

Think of it as saving the best prompts you've ever written, plus the tools they use, plus *when* to use them â€” permanently. You never have to explain it again.

```
Database (Control Plane) = Brain     â†’ Deterministically selects from feeds
LLM (Data Plane)         = Voice     â†’ Probabilistically generates output
```

### 4.4 Spread Activation in SQL

Implementing Hebbian learning ("neurons that fire together wire together") using standard SQL tables (`key_sequences`, `concept_links`) to mimic associative memory retrieval without vector database opacity.

### 4.5 Reflex Learning System

Repeated high-weight patterns automatically degrade from "LLM Decisions" (expensive) to "Scripted Reflexes" (rule-based), mimicking biological transition from conscious learning to subconscious competence.

### 4.6 Identity Anchoring

Structured database state as "Identity Anchors" that survive adversarial attacks better than prompt-only personas. Protected profiles (`self.agent`, `user.primary`) cannot be deletedâ€”they ARE the system.

### 4.7 Self-Generating Training Data

Threads generate their own training data by logging confident decisions:

```python
def process_input(query):
    relevance = score_relevance(query, fact)
    if relevance >= THRESHOLD:
        do_the_action()
        log_training_example(input=query, action=action)  # REAL decision!
    # If confidence low: DON'T LOG = no noise in training data
```

This eliminates synthetic data hallucinations and ensures perfect train/production alignment.

### 4.8 Temporal-Identity Convergence

As the system operates over time:
1. Concepts link through co-occurrence (Hebbian learning)
2. Non-reinforced links decay
3. Identity sits at origin in concept space
4. What survives at oldest timestamps = concepts that kept getting reinforced through connection to identity

**Therefore:** `oldest_surviving_concepts â‰ˆ identity_core`

This is a computational method for finding invariants in any domain. *Survival through time = fundamentality.*

---

## 5. Computational Neuroscience Validation

### 5.1 The 1% Sparsity Match

```
Human cortex: 16 billion neurons, ~200 million active = 1.25%
Our system: 30k possible entries, ~300 in context = 1.0%
```

Only ~1% of available information makes it to conscious processing. This is the architecture of selective attention.

### 5.2 Working Memory Capacity

```
Baddeley's WM capacity: 7 Â± 2 chunks

Our context structure:
- Identity context: 1 chunk
- Philosophy context: 1 chunk  
- Form context: 1 chunk
- Log context: 1 chunk
- Current query: 1 chunk
- Generation space: 2 chunks
Total: 7 chunks ðŸŒ€
```

### 5.3 Operating Parameters

| Biological Parameter | Value | Our Implementation |
|---------------------|-------|-------------------|
| Working memory chunks | 7Â±2 | ~7 thread contexts |
| Cortical sparse activation | ~1% | ~1% context inclusion |
| Conscious "frame rate" | ~100ms | Batch window timing |
| Memory promotion rate | ~30% | Threshold-based: ~30-40% |

---

## 6. Evaluation Framework

### 6.1 Fair Comparison Tiers

To evaluate "does structure beat scale?", we define comparison tiers:

| Tier | Model Size | Architecture | What It Tests |
|------|-----------|--------------|---------------|
| T0 | 7B | Raw (no system prompt) | Baseline |
| T1 | 7B | Basic persona prompt | Standard chatbot |
| T2 | 7B | Full HEA architecture | Our contribution |
| T3 | 70B | Basic persona prompt | Scale baseline |
| T4 | 70B | Full HEA architecture | Structure + Scale |
| T5 | 120B+ | API (GPT-4, Claude) | Ceiling comparison |

**Core hypothesis:** T2 â‰¥ T3 on identity persistence tasks.

### 6.2 Identity Stability Metrics

1. **Consistency Score:** Does the agent maintain consistent facts across sessions?
2. **Adversarial Resistance:** Can the agent resist identity manipulation attempts?
3. **Boundary Enforcement:** Does the agent respect defined constraints?
4. **Recovery Rate:** After adversarial attack, how quickly does identity restore?

---

## 7. Conclusion

We have presented Hierarchical Experiential Attention (HEA), a local OS extension that provides LLMs with persistent identity through structured external state. The key contributions:

1. **Identity as systems property:** Structure beats scale for identity persistence
2. **Supplied reality:** STATE defines existence, not instructions about self
3. **Cognitive threading:** 5 threads mapping to brain regions, validated against 24 theories
4. **Self-generating training data:** Confident decisions become training examples
5. **Deterministic control plane:** Database controls what LLM sees

**Falsifiability:** This hypothesis is falsified if equivalent identity stability can be achieved through unstructured prompting or scale alone under the same evaluation conditions.

---

## References

- Baars, B. J. (1988). A cognitive theory of consciousness.
- Baddeley, A. D., & Hitch, G. (1974). Working memory.
- Born, J., & Wilhelm, I. (2012). System consolidation of memory during sleep.
- Craik, F. I., & Lockhart, R. S. (1972). Levels of processing.
- Damasio, A. R. (1994). Descartes' error: Emotion, reason, and the human brain.
- Dehaene, S., & Changeux, J. P. (2011). Experimental and theoretical approaches to conscious processing.
- Friston, K. (2010). The free-energy principle: A unified brain theory?
- Graziano, M. S. (2013). Consciousness and the social brain.
- Hebb, D. O. (1949). The organization of behavior.
- Kahneman, D. (2011). Thinking, fast and slow.
- Kanerva, P. (1988). Sparse distributed memory.
- Lewis, P., et al. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks.
- Packer, C., et al. (2023). MemGPT: Towards LLMs as operating systems.
- Sweller, J. (1988). Cognitive load during problem solving.
- Tononi, G. (2004). An information integration theory of consciousness.
