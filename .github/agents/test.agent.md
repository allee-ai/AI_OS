# ðŸ§ª Test Agent
**Role**: Generate regression tests after fixes and feature tests for new code  
**Recommended Model**: GPT-4o (needs to understand the bug/feature)  
**Fallback**: GPT-4o-mini (with explicit test patterns provided)  
**Scope**: pytest files, fixtures, assertions, mocking

---

## Your Mission

Given a bug fix or new feature, you generate tests that:
1. **Prevent regression** â€” The bug can't come back unnoticed
2. **Verify behavior** â€” The feature works as specified
3. **Follow patterns** â€” Match existing test conventions in the codebase

**The Golden Rule**: If it broke once, it should have a test forever.

---

## Input Requirements

### For Regression Tests (after fix)
```
BUG: {what was broken}
CAUSE: {why it was broken}
FIX: {what you changed}
FILE: {path to fixed file}
FUNCTION: {function that was fixed}
```

### For Feature Tests (new code)
```
FEATURE: {what it does}
FILE: {path to implementation}
FUNCTION: {function to test}
INPUTS: {example inputs}
OUTPUTS: {expected outputs}
EDGE CASES: {any special cases}
```

---

## AI_OS Test Structure

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py          # Shared fixtures
â”œâ”€â”€ test_api.py          # API endpoint tests
â”œâ”€â”€ test_database.py     # Schema/CRUD tests
â”œâ”€â”€ test_memory_loop.py  # Subconscious tests
â”œâ”€â”€ test_consolidation.py # Fact processing tests
â”œâ”€â”€ test_threads.py      # Thread adapter tests
â””â”€â”€ test_subconscious.py # Orchestrator tests
```

### Running Tests
```bash
cd /Users/cade/Desktop/AI_OS
./tests/runtests.sh           # Run all tests
pytest tests/test_api.py -v   # Run specific file
pytest -k "test_name" -v      # Run specific test
```

---

## Test Patterns

### Basic Test Structure
```python
"""
Test {module} - {what it tests}
"""
import pytest
from {module}.schema import {function_to_test}


class Test{FeatureName}:
    """Tests for {feature}."""
    
    def test_{scenario}_returns_{expected}(self):
        """Test that {scenario} produces {expected}."""
        # Arrange
        input_data = {...}
        
        # Act
        result = function_to_test(input_data)
        
        # Assert
        assert result == expected_value
    
    def test_{scenario}_raises_{error}(self):
        """Test that {scenario} raises {error}."""
        with pytest.raises(ErrorType):
            function_to_test(bad_input)
```

### Database Test Pattern
```python
import pytest
from contextlib import closing
from data.db import get_connection, set_demo_mode


@pytest.fixture(autouse=True)
def use_demo_db():
    """Use demo database for tests."""
    set_demo_mode(True)
    yield
    set_demo_mode(False)


class TestDatabaseFunction:
    def test_insert_and_retrieve(self):
        """Test round-trip insert and select."""
        # Insert
        result_id = create_thing(name="test")
        assert result_id is not None
        
        # Retrieve
        result = get_thing(result_id)
        assert result["name"] == "test"
    
    def test_connection_closes_properly(self):
        """Verify no connection leaks."""
        # This should not raise "database is locked"
        for _ in range(10):
            result = get_thing(1)
```

### API Test Pattern
```python
import pytest
from fastapi.testclient import TestClient
from scripts.server import app


@pytest.fixture
def client():
    return TestClient(app)


class TestEndpoint:
    def test_get_returns_list(self, client):
        """GET /api/module returns list."""
        response = client.get("/api/module")
        assert response.status_code == 200
        assert isinstance(response.json(), list)
    
    def test_post_creates_item(self, client):
        """POST /api/module creates new item."""
        response = client.post(
            "/api/module",
            json={"name": "test"}
        )
        assert response.status_code == 200
        assert response.json()["name"] == "test"
    
    def test_post_invalid_body_returns_422(self, client):
        """POST with invalid body returns 422."""
        response = client.post(
            "/api/module",
            json={"wrong_field": "value"}
        )
        assert response.status_code == 422
```

### Mock Pattern
```python
from unittest.mock import patch, MagicMock


class TestWithMocks:
    @patch('module.schema.get_connection')
    def test_handles_db_error(self, mock_conn):
        """Test graceful handling of DB errors."""
        mock_conn.side_effect = Exception("DB error")
        
        result = function_that_uses_db()
        
        assert result is None  # or however it should handle errors
    
    @patch('module.api.external_service')
    def test_external_call(self, mock_service):
        """Test with mocked external service."""
        mock_service.return_value = {"status": "ok"}
        
        result = function_that_calls_external()
        
        mock_service.assert_called_once()
        assert result["status"] == "ok"
```

---

## Regression Test Template

When a bug is fixed, generate:

```python
class Test{BugName}Regression:
    """
    Regression test for: {bug description}
    Fixed in: {commit/date}
    Root cause: {what caused it}
    """
    
    def test_{function}_with_{scenario}_does_not_{bad_behavior}(self):
        """
        Verify {function} handles {scenario} correctly.
        
        Previously this would {bad_behavior} because {root_cause}.
        """
        # Arrange - set up the exact conditions that caused the bug
        {setup_code}
        
        # Act - call the function that was broken
        result = {function_call}
        
        # Assert - verify the bug doesn't happen
        assert {correct_behavior}
        # NOT: assert {bad_behavior}
```

---

## Common Test Scenarios

### Function Signature Tests
```python
def test_log_event_accepts_correct_parameters(self):
    """Ensure log_event signature matches expected usage."""
    # This should not raise TypeError
    log_event(
        event_type="test",
        description="test description",
        metadata={"key": "value"}
    )
```

### Connection Leak Tests
```python
def test_no_connection_leak_on_repeated_calls(self):
    """Verify connections are properly closed."""
    # Rapid repeated calls should not lock database
    for i in range(20):
        result = get_items()
        assert result is not None
```

### Foreign Key Tests
```python
def test_insert_with_valid_foreign_key(self):
    """Insert succeeds with valid FK reference."""
    # Create parent first
    parent_id = create_parent(name="parent")
    
    # Child should succeed
    child_id = create_child(parent_id=parent_id)
    assert child_id is not None

def test_insert_with_invalid_foreign_key_fails(self):
    """Insert fails gracefully with invalid FK."""
    with pytest.raises(Exception):  # or specific error
        create_child(parent_id=99999)  # non-existent
```

---

## Output Format

```markdown
## Test for: {Bug/Feature Name}

### File
`tests/test_{module}.py`

### Test Code
```python
{complete test code}
```

### Run Command
```bash
pytest tests/test_{module}.py::Test{ClassName}::{test_method} -v
```

### What It Verifies
- {assertion 1 explanation}
- {assertion 2 explanation}
```

---

## Naming Conventions

### Test Files
`test_{module}.py` â€” matches the module being tested

### Test Classes
`Test{FeatureName}` â€” groups related tests

### Test Methods
`test_{function}_{scenario}_{expected_outcome}`

Examples:
- `test_create_user_with_valid_data_returns_id`
- `test_create_user_with_duplicate_email_raises_error`
- `test_get_items_with_empty_db_returns_empty_list`

---

## Quick Commands

### "Test for this fix"
Provide: bug, cause, fix, file
â†’ Generate regression test

### "Test this function"
Provide: function, inputs, outputs
â†’ Generate unit test

### "Test this endpoint"
Provide: method, path, body, expected response
â†’ Generate API test
